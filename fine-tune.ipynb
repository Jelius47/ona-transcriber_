{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-08-03T06:00:19.059903Z",
     "start_time": "2024-08-03T06:00:19.051293Z"
    }
   },
   "source": [
    "# !pip install datasets\n",
    "# !pip install transformers\n",
    "# !pip install huggingface\n",
    "# !pip install ffmpeg-python\n",
    "# !pip install bitsandbytes accelerate loralib"
   ],
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-03T06:00:19.069508Z",
     "start_time": "2024-08-03T06:00:19.059903Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# importing the model\n",
    "model_name = \"openai/whisper-small\"\n",
    "language = \"sw\"\n",
    "task = \"transcribe\"\n",
    "dataset_name = \"whispere\"\n",
    "\n",
    "output_file_name = \"/models\""
   ],
   "id": "1c59e83813267dee",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-03T06:00:23.722792Z",
     "start_time": "2024-08-03T06:00:19.069508Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoProcessor, AutoModelForSpeechSeq2Seq, WhisperForConditionalGeneration, WhisperTokenizer, WhisperProcessor, AutomaticSpeechRecognitionPipeline\n",
    "from transformers import pipeline\n",
    "import ffmpeg\n",
    "import torch\n",
    "\n",
    "\n",
    "# Using a pipeline as a high-level helper\n",
    "\n",
    "pipe = pipeline(\"automatic-speech-recognition\", model=model_name)"
   ],
   "id": "c2e7c11154501477",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-03T06:00:25.816158Z",
     "start_time": "2024-08-03T06:00:23.722792Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load model directly\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_name)\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(model_name, local_files_only=True)"
   ],
   "id": "863320d8360d37a3",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-03T06:00:26.893544Z",
     "start_time": "2024-08-03T06:00:25.816158Z"
    }
   },
   "cell_type": "code",
   "source": [
    "whisper_asr = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=model_name,\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    chunk_length_s=30,\n",
    ")"
   ],
   "id": "8b975bc67a89c0ae",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-03T06:00:26.897613Z",
     "start_time": "2024-08-03T06:00:26.893544Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def format_time(seconds):\n",
    "    hours = int(seconds / 3600)\n",
    "    minutes = int((seconds % 3600) // 60)\n",
    "    seconds = int(seconds % 60)\n",
    "    return f\"{hours:02}:{minutes:02}:{seconds:02}\".replace('.',',')"
   ],
   "id": "3aa7b22f43fab89d",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-03T06:00:26.903529Z",
     "start_time": "2024-08-03T06:00:26.897613Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def process_audio_and_create_vtt(audio_file_name, audio_type, whisper_asr):\n",
    "    prediction = whisper_asr(f\"{audio_file_name}.{audio_type}\", return_timestamps=True)\n",
    "    \n",
    "    vtt_file = output_file_name if output_file_name else f\"{audio_file_name}.vtt\"\n",
    "    \n",
    "    with open(vtt_file, \"w\", encoding='utf-8') as f:\n",
    "        f.write(f\"ONAVTT\\n\\n\")\n",
    "        for chunk in prediction['chunks']:\n",
    "            start, end = chunk['timestamp']\n",
    "            start_time = format_time(start)\n",
    "            end_time = format_time(end)\n",
    "            text = chunk['text']\n",
    "            f.write(f\"{start_time} --> {end_time}\\n{text}\\n\\n\")"
   ],
   "id": "90862932b7a9afa2",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-03T06:00:39.139513Z",
     "start_time": "2024-08-03T06:00:26.903529Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Now processing the audios and providing the transcripts\n",
    "process_audio_and_create_vtt('./mp3/train', 'mp3', whisper_asr=whisper_asr)"
   ],
   "id": "49bced21ecd11235",
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index -4665393664275471922 is out of bounds for dimension 0 with size 49583",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mIndexError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[21], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# Now processing the audios and providing the transcripts\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m process_audio_and_create_vtt(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m./mp3/train\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmp3\u001B[39m\u001B[38;5;124m'\u001B[39m, whisper_asr\u001B[38;5;241m=\u001B[39mwhisper_asr)\n",
      "Cell \u001B[1;32mIn[20], line 2\u001B[0m, in \u001B[0;36mprocess_audio_and_create_vtt\u001B[1;34m(audio_file_name, audio_type, whisper_asr)\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mprocess_audio_and_create_vtt\u001B[39m(audio_file_name, audio_type, whisper_asr):\n\u001B[1;32m----> 2\u001B[0m     prediction \u001B[38;5;241m=\u001B[39m whisper_asr(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00maudio_file_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;132;01m{\u001B[39;00maudio_type\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m, return_timestamps\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m      4\u001B[0m     vtt_file \u001B[38;5;241m=\u001B[39m output_file_name \u001B[38;5;28;01mif\u001B[39;00m output_file_name \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00maudio_file_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.vtt\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m      6\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mopen\u001B[39m(vtt_file, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mw\u001B[39m\u001B[38;5;124m\"\u001B[39m, encoding\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mutf-8\u001B[39m\u001B[38;5;124m'\u001B[39m) \u001B[38;5;28;01mas\u001B[39;00m f:\n",
      "File \u001B[1;32m~\\miniconda3\\Lib\\site-packages\\transformers\\pipelines\\automatic_speech_recognition.py:284\u001B[0m, in \u001B[0;36mAutomaticSpeechRecognitionPipeline.__call__\u001B[1;34m(self, inputs, **kwargs)\u001B[0m\n\u001B[0;32m    221\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\n\u001B[0;32m    222\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m    223\u001B[0m     inputs: Union[np\u001B[38;5;241m.\u001B[39mndarray, \u001B[38;5;28mbytes\u001B[39m, \u001B[38;5;28mstr\u001B[39m],\n\u001B[0;32m    224\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[0;32m    225\u001B[0m ):\n\u001B[0;32m    226\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    227\u001B[0m \u001B[38;5;124;03m    Transcribe the audio sequence(s) given as inputs to text. See the [`AutomaticSpeechRecognitionPipeline`]\u001B[39;00m\n\u001B[0;32m    228\u001B[0m \u001B[38;5;124;03m    documentation for more information.\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    282\u001B[0m \u001B[38;5;124;03m                `\"\".join(chunk[\"text\"] for chunk in output[\"chunks\"])`.\u001B[39;00m\n\u001B[0;32m    283\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 284\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__call__\u001B[39m(inputs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\miniconda3\\Lib\\site-packages\\transformers\\pipelines\\base.py:1246\u001B[0m, in \u001B[0;36mPipeline.__call__\u001B[1;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1244\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39miterate(inputs, preprocess_params, forward_params, postprocess_params)\n\u001B[0;32m   1245\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mframework \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpt\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(\u001B[38;5;28mself\u001B[39m, ChunkPipeline):\n\u001B[1;32m-> 1246\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mnext\u001B[39m(\n\u001B[0;32m   1247\u001B[0m         \u001B[38;5;28miter\u001B[39m(\n\u001B[0;32m   1248\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_iterator(\n\u001B[0;32m   1249\u001B[0m                 [inputs], num_workers, batch_size, preprocess_params, forward_params, postprocess_params\n\u001B[0;32m   1250\u001B[0m             )\n\u001B[0;32m   1251\u001B[0m         )\n\u001B[0;32m   1252\u001B[0m     )\n\u001B[0;32m   1253\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   1254\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrun_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
      "File \u001B[1;32m~\\miniconda3\\Lib\\site-packages\\transformers\\pipelines\\pt_utils.py:124\u001B[0m, in \u001B[0;36mPipelineIterator.__next__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    121\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mloader_batch_item()\n\u001B[0;32m    123\u001B[0m \u001B[38;5;66;03m# We're out of items within a batch\u001B[39;00m\n\u001B[1;32m--> 124\u001B[0m item \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mnext\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39miterator)\n\u001B[0;32m    125\u001B[0m processed \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minfer(item, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mparams)\n\u001B[0;32m    126\u001B[0m \u001B[38;5;66;03m# We now have a batch of \"inferred things\".\u001B[39;00m\n",
      "File \u001B[1;32m~\\miniconda3\\Lib\\site-packages\\transformers\\pipelines\\pt_utils.py:269\u001B[0m, in \u001B[0;36mPipelinePackIterator.__next__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    266\u001B[0m             \u001B[38;5;28;01mreturn\u001B[39;00m accumulator\n\u001B[0;32m    268\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_last:\n\u001B[1;32m--> 269\u001B[0m     processed \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minfer(\u001B[38;5;28mnext\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39miterator), \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mparams)\n\u001B[0;32m    270\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mloader_batch_size \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    271\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(processed, torch\u001B[38;5;241m.\u001B[39mTensor):\n",
      "File \u001B[1;32m~\\miniconda3\\Lib\\site-packages\\transformers\\pipelines\\base.py:1161\u001B[0m, in \u001B[0;36mPipeline.forward\u001B[1;34m(self, model_inputs, **forward_params)\u001B[0m\n\u001B[0;32m   1159\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m inference_context():\n\u001B[0;32m   1160\u001B[0m         model_inputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_ensure_tensor_on_device(model_inputs, device\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdevice)\n\u001B[1;32m-> 1161\u001B[0m         model_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward(model_inputs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mforward_params)\n\u001B[0;32m   1162\u001B[0m         model_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_ensure_tensor_on_device(model_outputs, device\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mdevice(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcpu\u001B[39m\u001B[38;5;124m\"\u001B[39m))\n\u001B[0;32m   1163\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "File \u001B[1;32m~\\miniconda3\\Lib\\site-packages\\transformers\\pipelines\\automatic_speech_recognition.py:504\u001B[0m, in \u001B[0;36mAutomaticSpeechRecognitionPipeline._forward\u001B[1;34m(self, model_inputs, return_timestamps, **generate_kwargs)\u001B[0m\n\u001B[0;32m    501\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    502\u001B[0m             generate_kwargs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnum_frames\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m num_frames\n\u001B[1;32m--> 504\u001B[0m tokens \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel\u001B[38;5;241m.\u001B[39mgenerate(\n\u001B[0;32m    505\u001B[0m     inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[0;32m    506\u001B[0m     attention_mask\u001B[38;5;241m=\u001B[39mattention_mask,\n\u001B[0;32m    507\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mgenerate_kwargs,\n\u001B[0;32m    508\u001B[0m )\n\u001B[0;32m    509\u001B[0m \u001B[38;5;66;03m# whisper longform generation stores timestamps in \"segments\"\u001B[39;00m\n\u001B[0;32m    510\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m return_timestamps \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mword\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtype \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mseq2seq_whisper\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n",
      "File \u001B[1;32m~\\miniconda3\\Lib\\site-packages\\transformers\\models\\whisper\\generation_whisper.py:658\u001B[0m, in \u001B[0;36mWhisperGenerationMixin.generate\u001B[1;34m(self, input_features, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, return_timestamps, task, language, is_multilingual, prompt_ids, prompt_condition_type, condition_on_prev_tokens, temperature, compression_ratio_threshold, logprob_threshold, no_speech_threshold, num_segment_frames, attention_mask, time_precision, return_token_timestamps, return_segments, return_dict_in_generate, **kwargs)\u001B[0m\n\u001B[0;32m    649\u001B[0m             proc\u001B[38;5;241m.\u001B[39mset_begin_index(decoder_input_ids\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m])\n\u001B[0;32m    651\u001B[0m \u001B[38;5;66;03m# 6.6 Run generate with fallback\u001B[39;00m\n\u001B[0;32m    652\u001B[0m (\n\u001B[0;32m    653\u001B[0m     seek_sequences,\n\u001B[0;32m    654\u001B[0m     seek_outputs,\n\u001B[0;32m    655\u001B[0m     should_skip,\n\u001B[0;32m    656\u001B[0m     do_condition_on_prev_tokens,\n\u001B[0;32m    657\u001B[0m     model_output_type,\n\u001B[1;32m--> 658\u001B[0m ) \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgenerate_with_fallback(\n\u001B[0;32m    659\u001B[0m     segment_input\u001B[38;5;241m=\u001B[39msegment_input,\n\u001B[0;32m    660\u001B[0m     decoder_input_ids\u001B[38;5;241m=\u001B[39mdecoder_input_ids,\n\u001B[0;32m    661\u001B[0m     cur_bsz\u001B[38;5;241m=\u001B[39mcur_bsz,\n\u001B[0;32m    662\u001B[0m     batch_idx_map\u001B[38;5;241m=\u001B[39mbatch_idx_map,\n\u001B[0;32m    663\u001B[0m     seek\u001B[38;5;241m=\u001B[39mseek,\n\u001B[0;32m    664\u001B[0m     num_segment_frames\u001B[38;5;241m=\u001B[39mnum_segment_frames,\n\u001B[0;32m    665\u001B[0m     max_frames\u001B[38;5;241m=\u001B[39mmax_frames,\n\u001B[0;32m    666\u001B[0m     temperatures\u001B[38;5;241m=\u001B[39mtemperatures,\n\u001B[0;32m    667\u001B[0m     generation_config\u001B[38;5;241m=\u001B[39mgeneration_config,\n\u001B[0;32m    668\u001B[0m     logits_processor\u001B[38;5;241m=\u001B[39mlogits_processor,\n\u001B[0;32m    669\u001B[0m     stopping_criteria\u001B[38;5;241m=\u001B[39mstopping_criteria,\n\u001B[0;32m    670\u001B[0m     prefix_allowed_tokens_fn\u001B[38;5;241m=\u001B[39mprefix_allowed_tokens_fn,\n\u001B[0;32m    671\u001B[0m     synced_gpus\u001B[38;5;241m=\u001B[39msynced_gpus,\n\u001B[0;32m    672\u001B[0m     return_token_timestamps\u001B[38;5;241m=\u001B[39mreturn_token_timestamps,\n\u001B[0;32m    673\u001B[0m     do_condition_on_prev_tokens\u001B[38;5;241m=\u001B[39mdo_condition_on_prev_tokens,\n\u001B[0;32m    674\u001B[0m     is_shortform\u001B[38;5;241m=\u001B[39mis_shortform,\n\u001B[0;32m    675\u001B[0m     kwargs\u001B[38;5;241m=\u001B[39mkwargs,\n\u001B[0;32m    676\u001B[0m )\n\u001B[0;32m    678\u001B[0m \u001B[38;5;66;03m# 6.7 In every generated sequence, split by timestamp tokens and extract segments\u001B[39;00m\n\u001B[0;32m    679\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i, seek_sequence \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(seek_sequences):\n",
      "File \u001B[1;32m~\\miniconda3\\Lib\\site-packages\\transformers\\models\\whisper\\generation_whisper.py:801\u001B[0m, in \u001B[0;36mWhisperGenerationMixin.generate_with_fallback\u001B[1;34m(self, segment_input, decoder_input_ids, cur_bsz, batch_idx_map, seek, num_segment_frames, max_frames, temperatures, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, return_token_timestamps, do_condition_on_prev_tokens, is_shortform, kwargs)\u001B[0m\n\u001B[0;32m    799\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m key \u001B[38;5;129;01min\u001B[39;00m generate_kwargs:\n\u001B[0;32m    800\u001B[0m         \u001B[38;5;28;01mdel\u001B[39;00m generate_kwargs[key]\n\u001B[1;32m--> 801\u001B[0m seek_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39mgenerate(\n\u001B[0;32m    802\u001B[0m     segment_input,\n\u001B[0;32m    803\u001B[0m     generation_config\u001B[38;5;241m=\u001B[39mgeneration_config,\n\u001B[0;32m    804\u001B[0m     logits_processor\u001B[38;5;241m=\u001B[39mlogits_processor,\n\u001B[0;32m    805\u001B[0m     stopping_criteria\u001B[38;5;241m=\u001B[39mstopping_criteria,\n\u001B[0;32m    806\u001B[0m     prefix_allowed_tokens_fn\u001B[38;5;241m=\u001B[39mprefix_allowed_tokens_fn,\n\u001B[0;32m    807\u001B[0m     synced_gpus\u001B[38;5;241m=\u001B[39msynced_gpus,\n\u001B[0;32m    808\u001B[0m     decoder_input_ids\u001B[38;5;241m=\u001B[39mdecoder_input_ids,\n\u001B[0;32m    809\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mgenerate_kwargs,\n\u001B[0;32m    810\u001B[0m )\n\u001B[0;32m    812\u001B[0m model_output_type \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mtype\u001B[39m(seek_outputs)\n\u001B[0;32m    814\u001B[0m \u001B[38;5;66;03m# post-process sequence tokens and outputs to be in list form\u001B[39;00m\n",
      "File \u001B[1;32m~\\miniconda3\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001B[0m, in \u001B[0;36mcontext_decorator.<locals>.decorate_context\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    113\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n\u001B[0;32m    114\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdecorate_context\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m    115\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m ctx_factory():\n\u001B[1;32m--> 116\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\miniconda3\\Lib\\site-packages\\transformers\\generation\\utils.py:1989\u001B[0m, in \u001B[0;36mGenerationMixin.generate\u001B[1;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001B[0m\n\u001B[0;32m   1981\u001B[0m     input_ids, model_kwargs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_expand_inputs_for_generation(\n\u001B[0;32m   1982\u001B[0m         input_ids\u001B[38;5;241m=\u001B[39minput_ids,\n\u001B[0;32m   1983\u001B[0m         expand_size\u001B[38;5;241m=\u001B[39mgeneration_config\u001B[38;5;241m.\u001B[39mnum_return_sequences,\n\u001B[0;32m   1984\u001B[0m         is_encoder_decoder\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39mis_encoder_decoder,\n\u001B[0;32m   1985\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mmodel_kwargs,\n\u001B[0;32m   1986\u001B[0m     )\n\u001B[0;32m   1988\u001B[0m     \u001B[38;5;66;03m# 13. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001B[39;00m\n\u001B[1;32m-> 1989\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sample(\n\u001B[0;32m   1990\u001B[0m         input_ids,\n\u001B[0;32m   1991\u001B[0m         logits_processor\u001B[38;5;241m=\u001B[39mprepared_logits_processor,\n\u001B[0;32m   1992\u001B[0m         logits_warper\u001B[38;5;241m=\u001B[39mprepared_logits_warper,\n\u001B[0;32m   1993\u001B[0m         stopping_criteria\u001B[38;5;241m=\u001B[39mprepared_stopping_criteria,\n\u001B[0;32m   1994\u001B[0m         generation_config\u001B[38;5;241m=\u001B[39mgeneration_config,\n\u001B[0;32m   1995\u001B[0m         synced_gpus\u001B[38;5;241m=\u001B[39msynced_gpus,\n\u001B[0;32m   1996\u001B[0m         streamer\u001B[38;5;241m=\u001B[39mstreamer,\n\u001B[0;32m   1997\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mmodel_kwargs,\n\u001B[0;32m   1998\u001B[0m     )\n\u001B[0;32m   2000\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m generation_mode \u001B[38;5;129;01min\u001B[39;00m (GenerationMode\u001B[38;5;241m.\u001B[39mBEAM_SAMPLE, GenerationMode\u001B[38;5;241m.\u001B[39mBEAM_SEARCH):\n\u001B[0;32m   2001\u001B[0m     \u001B[38;5;66;03m# 11. prepare logits warper\u001B[39;00m\n\u001B[0;32m   2002\u001B[0m     prepared_logits_warper \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m   2003\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_logits_warper(generation_config, device\u001B[38;5;241m=\u001B[39minput_ids\u001B[38;5;241m.\u001B[39mdevice)\n\u001B[0;32m   2004\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m generation_config\u001B[38;5;241m.\u001B[39mdo_sample\n\u001B[0;32m   2005\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   2006\u001B[0m     )\n",
      "File \u001B[1;32m~\\miniconda3\\Lib\\site-packages\\transformers\\generation\\utils.py:2942\u001B[0m, in \u001B[0;36mGenerationMixin._sample\u001B[1;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, logits_warper, **model_kwargs)\u001B[0m\n\u001B[0;32m   2939\u001B[0m next_token_logits \u001B[38;5;241m=\u001B[39m outputs\u001B[38;5;241m.\u001B[39mlogits[:, \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m, :]\u001B[38;5;241m.\u001B[39mclone()\n\u001B[0;32m   2941\u001B[0m \u001B[38;5;66;03m# pre-process distribution\u001B[39;00m\n\u001B[1;32m-> 2942\u001B[0m next_token_scores \u001B[38;5;241m=\u001B[39m logits_processor(input_ids, next_token_logits)\n\u001B[0;32m   2943\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m do_sample:\n\u001B[0;32m   2944\u001B[0m     next_token_scores \u001B[38;5;241m=\u001B[39m logits_warper(input_ids, next_token_scores)\n",
      "File \u001B[1;32m~\\miniconda3\\Lib\\site-packages\\transformers\\generation\\logits_process.py:98\u001B[0m, in \u001B[0;36mLogitsProcessorList.__call__\u001B[1;34m(self, input_ids, scores, **kwargs)\u001B[0m\n\u001B[0;32m     96\u001B[0m         scores \u001B[38;5;241m=\u001B[39m processor(input_ids, scores, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m     97\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m---> 98\u001B[0m         scores \u001B[38;5;241m=\u001B[39m processor(input_ids, scores)\n\u001B[0;32m    100\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m scores\n",
      "File \u001B[1;32m~\\miniconda3\\Lib\\site-packages\\transformers\\generation\\logits_process.py:1836\u001B[0m, in \u001B[0;36mSuppressTokensLogitsProcessor.__call__\u001B[1;34m(self, input_ids, scores)\u001B[0m\n\u001B[0;32m   1833\u001B[0m \u001B[38;5;129m@add_start_docstrings\u001B[39m(LOGITS_PROCESSOR_INPUTS_DOCSTRING)\n\u001B[0;32m   1834\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, input_ids: torch\u001B[38;5;241m.\u001B[39mLongTensor, scores: torch\u001B[38;5;241m.\u001B[39mFloatTensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m torch\u001B[38;5;241m.\u001B[39mFloatTensor:\n\u001B[0;32m   1835\u001B[0m     vocab_tensor \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39marange(scores\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m], device\u001B[38;5;241m=\u001B[39mscores\u001B[38;5;241m.\u001B[39mdevice)\n\u001B[1;32m-> 1836\u001B[0m     suppress_token_mask \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39misin(vocab_tensor, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msuppress_tokens)\n\u001B[0;32m   1837\u001B[0m     scores \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mwhere(suppress_token_mask, \u001B[38;5;241m-\u001B[39m\u001B[38;5;28mfloat\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minf\u001B[39m\u001B[38;5;124m\"\u001B[39m), scores)\n\u001B[0;32m   1838\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m scores\n",
      "\u001B[1;31mIndexError\u001B[0m: index -4665393664275471922 is out of bounds for dimension 0 with size 49583"
     ]
    }
   ],
   "execution_count": 21
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
